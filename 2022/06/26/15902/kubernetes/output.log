+ ALL_RESULT_DIR=/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result
+ rm '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/*'
rm: cannot remove '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/*': No such file or directory
+ true
+ mkdir -p /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result
+ RESULT=0
+ IFS='
'
++ find /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples -name test.sh
++ grep ''
++ sort
+ for test in $(find "$SCRIPT_DIR" -name test.sh | grep "${OZONE_TEST_SELECTOR:-""}" |sort)
++ dirname /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/test.sh
+ TEST_DIR=/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started
++ basename /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started
+ TEST_NAME=getting-started
+ echo ''

+ echo '#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started #####'
#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started #####
+ echo ''

+ cd /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started
+ ./test.sh

**** Modifying Kubernetes resources file for test ****

   (mounting current Ozone directory to the containers, scheduling containers to one node, ...)

WARNING: this test can be executed only with local Kubernetes cluster
   (source dir should be available from K8s nodes)


**** Deleting existing k8s resources ****

No resources found
No resources found
No resources found
service "kubernetes" deleted
configmap "kube-root-ca.crt" deleted
No resources found
No resources found
No resources found

**** Applying k8s resources from getting-started ****

configmap/config created
service/datanode-public created
service/datanode created
statefulset.apps/datanode created
service/om-public created
service/om created
statefulset.apps/om created
service/s3g-public created
service/s3g created
statefulset.apps/s3g created
service/scm-public created
service/scm created
statefulset.apps/scm created
error: error validating "kustomization.yaml": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false

**** Waiting until the k8s cluster is running ****

No resources found in default namespace.
-1 pods are running. Waiting for more.
1 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
2 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
3 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
4 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
5 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
6 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
7 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
8 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
9 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
10 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
11 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
12 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
13 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
14 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
15 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
16 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
17 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
18 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
19 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
20 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
21 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
22 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
23 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
24 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
25 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
26 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
27 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
28 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
29 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
30 'all_pods_are_running' is failed...
4 pods are running out from the 6
31 'all_pods_are_running' is failed...
5 pods are running out from the 6
32 'all_pods_are_running' is failed...
5 pods are running out from the 6
33 'all_pods_are_running' is failed...
5 pods are running out from the 6
34 'all_pods_are_running' is failed...
5 pods are running out from the 6
35 'all_pods_are_running' is failed...
5 pods are running out from the 6
36 'all_pods_are_running' is failed...
Defaulted container "scm" out of: scm, init (init)
1 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
3 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
4 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
5 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
6 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
7 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
8 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
9 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
10 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2022-06-26 12:59:56 INFO  SCMSafeModeManager:248 - SCM exiting safe mode.
2022-06-26 12:59:52 INFO  BaseHttpServer:329 - HTTP server of ozoneManager listening at http://0.0.0.0:9874

**** Cluster is up and running ****


**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Basic :: Smoketest ozone cluster startup                                      
==============================================================================
Check webui static resources                                          | PASS |
------------------------------------------------------------------------------
Basic Freon smoketest                                                 | PASS |
------------------------------------------------------------------------------
Basic :: Smoketest ozone cluster startup                              | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names
rm: cannot remove 'result/output.xml': No such file or directory
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/result/output.xml
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/result/report.html
Defaulted container "scm" out of: scm, init (init)
configmap "config" deleted
service "datanode-public" deleted
service "datanode" deleted
statefulset.apps "datanode" deleted
service "om-public" deleted
service "om" deleted
statefulset.apps "om" deleted
service "s3g-public" deleted
service "s3g" deleted
statefulset.apps "s3g" deleted
service "scm-public" deleted
service "scm" deleted
statefulset.apps "scm" deleted
error: unable to decode "kustomization.yaml": Object 'Kind' is missing in '{"resources":["config-configmap.yaml","datanode-service.yaml","datanode-statefulset.yaml","om-service.yaml","om-statefulset.yaml","s3g-service.yaml","s3g-statefulset.yaml","scm-service.yaml","scm-statefulset.yaml","datanode-public-service.yaml","om-public-service.yaml","s3g-public-service.yaml","scm-public-service.yaml"]}'

**** Regenerating original Kubernetes resource files ****

+ cp /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/result/output.xml /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/getting-started.xml
+ mkdir -p /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/getting-started
+ mv /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-datanode-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-datanode-1.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-datanode-2.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-om-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-s3g-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-scm-0-init.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/getting-started/logs/pod-scm-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/getting-started/
+ for test in $(find "$SCRIPT_DIR" -name test.sh | grep "${OZONE_TEST_SELECTOR:-""}" |sort)
++ dirname /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/test.sh
+ TEST_DIR=/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube
++ basename /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube
+ TEST_NAME=minikube
+ echo ''

+ echo '#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube #####'
#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube #####
+ echo ''

+ cd /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube
+ ./test.sh

**** Modifying Kubernetes resources file for test ****

   (mounting current Ozone directory to the containers, scheduling containers to one node, ...)

WARNING: this test can be executed only with local Kubernetes cluster
   (source dir should be available from K8s nodes)


**** Deleting existing k8s resources ****

No resources found
No resources found
No resources found
service "kubernetes" deleted
configmap "kube-root-ca.crt" deleted
pod "datanode-0" deleted
pod "datanode-1" deleted
pod "datanode-2" deleted
pod "om-0" deleted
pod "s3g-0" deleted
pod "scm-0" deleted
No resources found
No resources found

**** Applying k8s resources from minikube ****

configmap/config created
service/datanode-public created
service/datanode created
statefulset.apps/datanode created
service/om-public created
service/om created
statefulset.apps/om created
service/s3g-public created
service/s3g created
statefulset.apps/s3g created
service/scm-public created
service/scm created
statefulset.apps/scm created
error: error validating "kustomization.yaml": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false

**** Waiting until the k8s cluster is running ****

No resources found in default namespace.
-1 pods are running. Waiting for more.
1 'all_pods_are_running' is failed...
3 pods are running out from the 6
2 'all_pods_are_running' is failed...
5 pods are running out from the 6
3 'all_pods_are_running' is failed...
5 pods are running out from the 6
4 'all_pods_are_running' is failed...
5 pods are running out from the 6
5 'all_pods_are_running' is failed...
5 pods are running out from the 6
6 'all_pods_are_running' is failed...
5 pods are running out from the 6
7 'all_pods_are_running' is failed...
Defaulted container "scm" out of: scm, init (init)
1 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
3 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
4 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
5 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
6 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
7 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
8 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
9 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
10 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
11 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
12 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
13 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
14 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2022-06-26 13:01:55 INFO  SCMSafeModeManager:248 - SCM exiting safe mode.
2022-06-26 13:01:38 INFO  BaseHttpServer:329 - HTTP server of ozoneManager listening at http://0.0.0.0:9874

**** Cluster is up and running ****


**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Basic :: Smoketest ozone cluster startup                                      
==============================================================================
Check webui static resources                                          | PASS |
------------------------------------------------------------------------------
Basic Freon smoketest                                                 | PASS |
------------------------------------------------------------------------------
Basic :: Smoketest ozone cluster startup                              | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names
rm: cannot remove 'result/output.xml': No such file or directory
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/result/output.xml
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/result/report.html
Defaulted container "scm" out of: scm, init (init)
configmap "config" deleted
service "datanode-public" deleted
service "datanode" deleted
statefulset.apps "datanode" deleted
service "om-public" deleted
service "om" deleted
statefulset.apps "om" deleted
service "s3g-public" deleted
service "s3g" deleted
statefulset.apps "s3g" deleted
service "scm-public" deleted
service "scm" deleted
statefulset.apps "scm" deleted
error: unable to decode "kustomization.yaml": Object 'Kind' is missing in '{"resources":["config-configmap.yaml","datanode-service.yaml","datanode-statefulset.yaml","om-service.yaml","om-statefulset.yaml","s3g-service.yaml","s3g-statefulset.yaml","scm-service.yaml","scm-statefulset.yaml","datanode-public-service.yaml","om-public-service.yaml","s3g-public-service.yaml","scm-public-service.yaml"]}'

**** Regenerating original Kubernetes resource files ****

+ cp /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/result/output.xml /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/minikube.xml
+ mkdir -p /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/minikube
+ mv /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-datanode-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-datanode-1.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-datanode-2.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-om-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-s3g-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-scm-0-init.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/minikube/logs/pod-scm-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/minikube/
+ for test in $(find "$SCRIPT_DIR" -name test.sh | grep "${OZONE_TEST_SELECTOR:-""}" |sort)
++ dirname /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/test.sh
+ TEST_DIR=/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev
++ basename /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev
+ TEST_NAME=ozone-dev
+ echo ''

+ echo '#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev #####'
#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev #####
+ echo ''

+ cd /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev
+ ./test.sh

**** Modifying Kubernetes resources file for test ****

   (mounting current Ozone directory to the containers, scheduling containers to one node, ...)

WARNING: this test can be executed only with local Kubernetes cluster
   (source dir should be available from K8s nodes)


**** Deleting existing k8s resources ****

No resources found
No resources found
No resources found
service "kubernetes" deleted
configmap "kube-root-ca.crt" deleted
pod "datanode-2" deleted
pod "datanode-1" deleted
pod "datanode-0" deleted
pod "om-0" deleted
pod "s3g-0" deleted
pod "scm-0" deleted
No resources found
No resources found

**** Applying k8s resources from ozone-dev ****

configmap/config created
service/datanode-public created
service/datanode created
statefulset.apps/datanode created
service/jaeger-public created
service/jaeger created
statefulset.apps/jaeger created
service/om-public created
service/om created
statefulset.apps/om created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/prometheus-default created
deployment.apps/prometheus created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator-default created
serviceaccount/prometheus-operator created
service/prometheus created
configmap/prometheusconf created
service/s3g-public created
service/s3g created
statefulset.apps/s3g created
service/scm-public created
service/scm created
statefulset.apps/scm created
error: error validating "kustomization.yaml": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false

**** Waiting until the k8s cluster is running ****

No resources found in default namespace.
-1 pods are running. Waiting for more.
1 'all_pods_are_running' is failed...
3 pods are running out from the 7
2 'all_pods_are_running' is failed...
6 pods are running out from the 8
3 'all_pods_are_running' is failed...
7 pods are running out from the 8
4 'all_pods_are_running' is failed...
7 pods are running out from the 8
5 'all_pods_are_running' is failed...
7 pods are running out from the 8
6 'all_pods_are_running' is failed...
7 pods are running out from the 8
7 'all_pods_are_running' is failed...
Defaulted container "scm" out of: scm, init (init)
1 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
3 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
4 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
5 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
6 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
7 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
8 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2022-06-26 13:03:37 INFO  SCMSafeModeManager:248 - SCM exiting safe mode.
1 'grep_log om-0 HTTP server of ozoneManager listening' is failed...
2022-06-26 13:03:39 INFO  BaseHttpServer:329 - HTTP server of ozoneManager listening at http://0.0.0.0:9874

**** Cluster is up and running ****


**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Basic :: Smoketest ozone cluster startup                                      
==============================================================================
Check webui static resources                                          | PASS |
------------------------------------------------------------------------------
Basic Freon smoketest                                                 | PASS |
------------------------------------------------------------------------------
Basic :: Smoketest ozone cluster startup                              | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names
rm: cannot remove 'result/output.xml': No such file or directory
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/result/output.xml
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/result/report.html
Defaulted container "scm" out of: scm, init (init)
configmap "config" deleted
service "datanode-public" deleted
service "datanode" deleted
statefulset.apps "datanode" deleted
service "jaeger-public" deleted
service "jaeger" deleted
statefulset.apps "jaeger" deleted
service "om-public" deleted
service "om" deleted
statefulset.apps "om" deleted
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io "prometheus-default" deleted
deployment.apps "prometheus" deleted
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io "prometheus-operator-default" deleted
serviceaccount "prometheus-operator" deleted
service "prometheus" deleted
configmap "prometheusconf" deleted
service "s3g-public" deleted
service "s3g" deleted
statefulset.apps "s3g" deleted
service "scm-public" deleted
service "scm" deleted
statefulset.apps "scm" deleted
error: unable to decode "kustomization.yaml": Object 'Kind' is missing in '{"resources":["config-configmap.yaml","datanode-service.yaml","datanode-statefulset.yaml","om-service.yaml","om-statefulset.yaml","s3g-service.yaml","s3g-statefulset.yaml","scm-service.yaml","scm-statefulset.yaml","prometheusconf-configmap.yaml","prometheus-deployment.yaml","prometheus-clusterrole.yaml","prometheus-operator-clusterrolebinding.yaml","prometheus-operator-serviceaccount.yaml","prometheus-service.yaml","jaeger-service.yaml","jaeger-statefulset.yaml","datanode-public-service.yaml","om-public-service.yaml","s3g-public-service.yaml","scm-public-service.yaml","jaeger-public-service.yaml"]}'

**** Regenerating original Kubernetes resource files ****

+ cp /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/result/output.xml /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ozone-dev.xml
+ mkdir -p /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ozone-dev
+ mv /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-datanode-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-datanode-1.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-datanode-2.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-jaeger-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-om-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-prometheus-54df6857dd-c6npz.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-s3g-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-scm-0-init.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone-dev/logs/pod-scm-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ozone-dev/
+ for test in $(find "$SCRIPT_DIR" -name test.sh | grep "${OZONE_TEST_SELECTOR:-""}" |sort)
++ dirname /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/test.sh
+ TEST_DIR=/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone
++ basename /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone
+ TEST_NAME=ozone
+ echo ''

+ echo '#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone #####'
#### Executing tests of /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone #####
+ echo ''

+ cd /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone
+ ./test.sh

**** Modifying Kubernetes resources file for test ****

   (mounting current Ozone directory to the containers, scheduling containers to one node, ...)

WARNING: this test can be executed only with local Kubernetes cluster
   (source dir should be available from K8s nodes)


**** Deleting existing k8s resources ****

No resources found
No resources found
No resources found
service "kubernetes" deleted
configmap "kube-root-ca.crt" deleted
pod "datanode-2" deleted
pod "datanode-1" deleted
pod "datanode-0" deleted
pod "jaeger-0" deleted
pod "prometheus-54df6857dd-c6npz" deleted
pod "s3g-0" deleted
pod "scm-0" deleted
pod "om-0" deleted
No resources found
No resources found

**** Applying k8s resources from ozone ****

configmap/config created
service/datanode created
statefulset.apps/datanode created
service/om created
statefulset.apps/om created
service/s3g created
statefulset.apps/s3g created
service/scm created
statefulset.apps/scm created
error: error validating "kustomization.yaml": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false

**** Waiting until the k8s cluster is running ****

No resources found in default namespace.
-1 pods are running. Waiting for more.
1 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
2 'all_pods_are_running' is failed...
No resources found in default namespace.
-1 pods are running. Waiting for more.
3 'all_pods_are_running' is failed...
1 pods are running. Waiting for more.
4 'all_pods_are_running' is failed...
2 pods are running. Waiting for more.
5 'all_pods_are_running' is failed...
3 pods are running out from the 5
6 'all_pods_are_running' is failed...
4 pods are running out from the 6
7 'all_pods_are_running' is failed...
5 pods are running out from the 6
8 'all_pods_are_running' is failed...
Defaulted container "scm" out of: scm, init (init)
1 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
3 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
4 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
5 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
6 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
7 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
8 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
9 'grep_log scm-0 SCM exiting safe mode.' is failed...
Defaulted container "scm" out of: scm, init (init)
2022-06-26 13:05:33 INFO  SCMSafeModeManager:248 - SCM exiting safe mode.
1 'grep_log om-0 HTTP server of ozoneManager listening' is failed...
2022-06-26 13:05:34 INFO  BaseHttpServer:329 - HTTP server of ozoneManager listening at http://0.0.0.0:9874

**** Cluster is up and running ****


**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Generate :: Test freon data generation commands                               
==============================================================================
Ozone Client Key Generator                                            | PASS |
------------------------------------------------------------------------------
OM Key Generator                                                      | PASS |
------------------------------------------------------------------------------
OM Bucket Generator                                                   | PASS |
------------------------------------------------------------------------------
Generate :: Test freon data generation commands                       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names

**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Validate :: Test freon data validation commands                               
==============================================================================
Ozone Client Key Validator                                            | PASS |
------------------------------------------------------------------------------
Validate :: Test freon data validation commands                       | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names
pod "datanode-0" deleted
pod "datanode-1" deleted
pod "datanode-2" deleted

**** Waiting until the k8s cluster is running ****

3 pods are running out from the 4
1 'all_pods_are_running' is failed...
4 pods are running out from the 5
2 'all_pods_are_running' is failed...
Defaulted container "scm" out of: scm, init (init)
2022-06-26 13:05:33 INFO  SCMSafeModeManager:248 - SCM exiting safe mode.
2022-06-26 13:05:34 INFO  BaseHttpServer:329 - HTTP server of ozoneManager listening at http://0.0.0.0:9874

**** Cluster is up and running ****


**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Validate :: Test freon data validation commands                               
==============================================================================
Ozone Client Key Validator                                            | FAIL |
255 != 0
------------------------------------------------------------------------------
Validate :: Test freon data validation commands                       | FAIL |
1 test, 0 passed, 1 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names

**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Generate :: Test freon data generation commands                               
==============================================================================
Ozone Client Key Generator                                            | PASS |
------------------------------------------------------------------------------
OM Key Generator                                                      | PASS |
------------------------------------------------------------------------------
OM Bucket Generator                                                   | PASS |
------------------------------------------------------------------------------
Generate :: Test freon data generation commands                       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names

**** Executing robot tests scm-0 ****

Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
Defaulted container "scm" out of: scm, init (init)
Unable to use a TTY - input is not a terminal or the right kind of file
==============================================================================
Validate :: Test freon data validation commands                               
==============================================================================
Ozone Client Key Validator                                            | PASS |
------------------------------------------------------------------------------
Validate :: Test freon data validation commands                       | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/report/output.xml
Log:     /tmp/report/log.html
Report:  /tmp/report/report.html
Defaulted container "scm" out of: scm, init (init)
tar: Removing leading `/' from member names
rm: cannot remove 'result/output.xml': No such file or directory
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/result/output.xml
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/result/report.html
Defaulted container "scm" out of: scm, init (init)
configmap "config" deleted
service "datanode" deleted
statefulset.apps "datanode" deleted
service "om" deleted
statefulset.apps "om" deleted
service "s3g" deleted
statefulset.apps "s3g" deleted
service "scm" deleted
statefulset.apps "scm" deleted
error: unable to decode "kustomization.yaml": Object 'Kind' is missing in '{"resources":["config-configmap.yaml","datanode-service.yaml","datanode-statefulset.yaml","om-service.yaml","om-statefulset.yaml","s3g-service.yaml","s3g-statefulset.yaml","scm-service.yaml","scm-statefulset.yaml"]}'

**** Regenerating original Kubernetes resource files ****

+ cp /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/result/output.xml /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ozone.xml
+ mkdir -p /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ozone
+ mv /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-datanode-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-datanode-1.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-datanode-2.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-om-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-s3g-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-scm-0-init.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/ozone/logs/pod-scm-0.log /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ozone/
+ rebot -N smoketests -d /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/ '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/*.xml'
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/kubernetes/examples/result/report.html
